{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Glue context and Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 10\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "  \n",
    "sc          = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark       = glueContext.spark_session\n",
    "job         = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_dynamic_frame_initial = glueContext.create_dynamic_frame.from_catalog(database='streaming_history', table_name='streaming_csv')\n",
    "\n",
    "df_spark = glue_dynamic_frame_initial.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df_spark.drop(\"username\") \\\n",
    "                     .drop(\"conn_country\") \\\n",
    "                     .drop(\"ip_addr_decrypted\") \\\n",
    "                     .drop(\"user_agent_decrypted\") \\\n",
    "                     .drop(\"spotify_track_uri\") \\\n",
    "                     .drop(\"episode_name\") \\\n",
    "                     .drop(\"episode_show_name\") \\\n",
    "                     .drop(\"spotify_episode_uri\") \\\n",
    "                     .drop(\"reason_start\") \\\n",
    "                     .drop(\"reason_end\") \\\n",
    "                     .drop(\"shuffle\") \\\n",
    "                     .drop(\"offline\") \\\n",
    "                     .drop(\"offline_timestamp\") \\\n",
    "                     .drop(\"incognito_mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename columns with proper name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df_dropped.withColumnRenamed(\"ts\", \"time_stamp\") \\\n",
    "                       .withColumnRenamed(\"platform\", \"device_used\") \\\n",
    "                       .withColumnRenamed(\"ms_played\", \"minutes_played\") \\\n",
    "                       .withColumnRenamed(\"master_metadata_track_name\", \"track_name\") \\\n",
    "                       .withColumnRenamed(\"master_metadata_album_artist_name\", \"artist_name\") \\\n",
    "                       .withColumnRenamed(\"master_metadata_album_album_name\", \"album_name\") \\\n",
    "                       .withColumnRenamed(\"skipped\", \"if_skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract month_name, date and time in hours from the data frame drop redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extract = df_renamed.withColumn(\"month_name\", F.date_format(F.col(\"time_stamp\"), \"MMMM\")) \\\n",
    "                       .withColumn(\"date\", F.date_format(F.col(\"time_stamp\"), \"yyyy-MM-dd\")) \\\n",
    "                       .withColumn('time_HH', F.date_format(F.col(\"time_stamp\"), \"HH\")) \\\n",
    "                       .drop(\"month\") \\\n",
    "                       .drop(\"time\") \\\n",
    "                       .drop(\"time_stamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Glue Dynamic Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Spark dataframe to glue dynamic frame\n",
    "glue_dynamic_frame_final = DynamicFrame.fromDF(df_extract, glueContext, \"glue_etl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
    "s3output = glueContext.getSink(\n",
    "    path=\"s3://aws-glue-job-spark-bucket/data/streaming_history/streaming_parquet/\",\n",
    "    connection_type=\"s3\",\n",
    "    updateBehavior=\"UPDATE_IN_DATABASE\",\n",
    "    partitionKeys=[],\n",
    "    compression=\"snappy\",\n",
    "    enableUpdateCatalog=True,\n",
    "    transformation_ctx=\"s3output\",\n",
    ")\n",
    "\n",
    "s3output.setCatalogInfo(\n",
    "  catalogDatabase=\"streaming_history\", catalogTableName=\"streaming_parquet\"\n",
    ")\n",
    "\n",
    "s3output.setFormat(\"glueparquet\")\n",
    "s3output.writeFrame(glue_dynamic_frame_final)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
